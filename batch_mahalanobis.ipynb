{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet()\n",
    "convs = []\n",
    "# list of a conv layers within alexnn\n",
    "for i,k in enumerate(model.modules()):\n",
    "    if isinstance(k, nn.Conv2d):\n",
    "        convs.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = convs[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(l1.shape[1]): # iterate over channels\n",
    "    X = torch.reshape(l1[0][j], [1,l1[0][j].shape[1]**2])  # gets first elem\n",
    "    for i,w in enumerate(l1): # iterates over filters\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            y = torch.reshape(w[0], [1,w[j].shape[1]**2])\n",
    "            X = torch.cat((X,y))\n",
    "# X is a matrix with each line being a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the covariance matrix of m\n",
    "def cov(m, rowvar=False):\n",
    "    if m.dim() > 2:\n",
    "        raise ValueError('m has more than 2 dimensions')\n",
    "    if m.dim() < 2:\n",
    "        m = m.view(1, -1)\n",
    "    if not rowvar and m.size(0) != 1:\n",
    "        m = m.t()\n",
    "    # m = m.type(torch.double)  # uncomment this line if desired\n",
    "    fact = 1.0 / (m.size(1) - 1)\n",
    "    m -= torch.mean(m, dim=1, keepdim=True)\n",
    "    mt = m.t()  # if complex: mt = m.t().conj()\n",
    "    return fact * m.matmul(mt).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_mahalanobis(L, x):\n",
    "    r\"\"\"\n",
    "    Computes the squared Mahalanobis distance :math:`\\mathbf{x}^\\top\\mathbf{M}^{-1}\\mathbf{x}`\n",
    "    for a factored :math:`\\mathbf{M} = \\mathbf{L}\\mathbf{L}^\\top`.\n",
    "\n",
    "    Accepts batches for both L and x.\n",
    "    \"\"\"\n",
    "    # TODO: use `torch.potrs` or similar once a backwards pass is implemented.\n",
    "    flat_L = L.unsqueeze(0).reshape((-1,) + L.shape[-2:])\n",
    "    L_inv = torch.stack([torch.inverse(Li.t()) for Li in flat_L]).view(L.shape)\n",
    "    return (x.unsqueeze(-1) * L_inv).sum(-2).pow(2.0).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(2.0762e-07, grad_fn=<SumBackward1>)\n",
      "tensor(1.3287e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(5.0059e-07, grad_fn=<SumBackward1>)\n",
      "tensor(6.2425e-06, grad_fn=<SumBackward1>)\n",
      "tensor(1.8221e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(1.8780e-08, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0001, grad_fn=<SumBackward1>)\n",
      "tensor(1.7564e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(4.3080e-06, grad_fn=<SumBackward1>)\n",
      "tensor(9.6521e-07, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(2.9630e-06, grad_fn=<SumBackward1>)\n",
      "tensor(8.1496e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(7.7386e-06, grad_fn=<SumBackward1>)\n",
      "tensor(8.4368e-08, grad_fn=<SumBackward1>)\n",
      "tensor(5.3094e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(1.1175e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(2.1205e-08, grad_fn=<SumBackward1>)\n",
      "tensor(0.0001, grad_fn=<SumBackward1>)\n",
      "tensor(5.0031e-08, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0001, grad_fn=<SumBackward1>)\n",
      "tensor(0.0001, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(5.8412e-06, grad_fn=<SumBackward1>)\n",
      "tensor(7.7174e-06, grad_fn=<SumBackward1>)\n",
      "tensor(9.6350e-06, grad_fn=<SumBackward1>)\n",
      "tensor(1.8175e-07, grad_fn=<SumBackward1>)\n",
      "tensor(8.8260e-07, grad_fn=<SumBackward1>)\n",
      "tensor(0.0001, grad_fn=<SumBackward1>)\n",
      "tensor(3.0552e-07, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(2.4683e-06, grad_fn=<SumBackward1>)\n",
      "tensor(2.6539e-06, grad_fn=<SumBackward1>)\n",
      "tensor(2.0010e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(1.3765e-07, grad_fn=<SumBackward1>)\n",
      "tensor(2.0532e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(1.8294e-06, grad_fn=<SumBackward1>)\n",
      "tensor(1.1814e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(7.3744e-07, grad_fn=<SumBackward1>)\n",
      "tensor(8.7589e-07, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n",
      "tensor(3.3907e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.3542e-06, grad_fn=<SumBackward1>)\n",
      "tensor(0.0000, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "VI = torch.inverse(cov(X)) #inverse of covariance matrix\n",
    "for _input in X:\n",
    "    print(_batch_mahalanobis(VI,_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
