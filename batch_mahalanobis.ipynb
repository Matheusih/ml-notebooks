{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet()\n",
    "convs = []\n",
    "# list of a conv layers within alexnn\n",
    "for i,k in enumerate(model.modules()):\n",
    "    if isinstance(k, nn.Conv2d):\n",
    "        convs.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = convs[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(l1.shape[1]): # iterate over channels\n",
    "    X = torch.reshape(l1[0][j], [1,l1[0][j].shape[1]**2])  # gets first elem\n",
    "    for i,w in enumerate(l1): # iterates over filters\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            y = torch.reshape(w[0], [1,w[j].shape[1]**2])\n",
    "            X = torch.cat((X,y))\n",
    "# X is a matrix with each line being a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the covariance matrix of m\n",
    "def cov(m, rowvar=False):\n",
    "    if m.dim() > 2:\n",
    "        raise ValueError('m has more than 2 dimensions')\n",
    "    if m.dim() < 2:\n",
    "        m = m.view(1, -1)\n",
    "    if not rowvar and m.size(0) != 1:\n",
    "        m = m.t()\n",
    "    # m = m.type(torch.double)  # uncomment this line if desired\n",
    "    fact = 1.0 / (m.size(1) - 1)\n",
    "    m -= torch.mean(m, dim=1, keepdim=True)\n",
    "    mt = m.t()  # if complex: mt = m.t().conj()\n",
    "    return fact * m.matmul(mt).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_mahalanobis(L, x):\n",
    "    r\"\"\"\n",
    "    Computes the squared Mahalanobis distance :math:`\\mathbf{x}^\\top\\mathbf{M}^{-1}\\mathbf{x}`\n",
    "    for a factored :math:`\\mathbf{M} = \\mathbf{L}\\mathbf{L}^\\top`.\n",
    "\n",
    "    Accepts batches for both L and x.\n",
    "    \"\"\"\n",
    "    # TODO: use `torch.potrs` or similar once a backwards pass is implemented.\n",
    "    flat_L = L.unsqueeze(0).reshape((-1,) + L.shape[-2:])\n",
    "    L_inv = torch.stack([torch.inverse(Li.t()) for Li in flat_L]).view(L.shape)\n",
    "    return (x.unsqueeze(-1) * L_inv).sum(-2).pow(2.0).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6430e-09, grad_fn=<SumBackward1>)\n",
      "tensor(7.5918e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.0314e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.2633e-07, grad_fn=<SumBackward1>)\n",
      "tensor(1.0388e-07, grad_fn=<SumBackward1>)\n",
      "tensor(2.4629e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.4290e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.3810e-08, grad_fn=<SumBackward1>)\n",
      "tensor(4.5422e-08, grad_fn=<SumBackward1>)\n",
      "tensor(7.3179e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.7271e-08, grad_fn=<SumBackward1>)\n",
      "tensor(9.4369e-09, grad_fn=<SumBackward1>)\n",
      "tensor(9.7419e-08, grad_fn=<SumBackward1>)\n",
      "tensor(5.8754e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.3643e-07, grad_fn=<SumBackward1>)\n",
      "tensor(1.9711e-07, grad_fn=<SumBackward1>)\n",
      "tensor(1.8443e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.4760e-08, grad_fn=<SumBackward1>)\n",
      "tensor(5.3511e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.0477e-08, grad_fn=<SumBackward1>)\n",
      "tensor(4.9527e-09, grad_fn=<SumBackward1>)\n",
      "tensor(5.4732e-08, grad_fn=<SumBackward1>)\n",
      "tensor(5.0620e-08, grad_fn=<SumBackward1>)\n",
      "tensor(3.1982e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.6130e-07, grad_fn=<SumBackward1>)\n",
      "tensor(6.0745e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.4350e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.6398e-08, grad_fn=<SumBackward1>)\n",
      "tensor(3.6016e-07, grad_fn=<SumBackward1>)\n",
      "tensor(1.6289e-08, grad_fn=<SumBackward1>)\n",
      "tensor(3.8048e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.6069e-07, grad_fn=<SumBackward1>)\n",
      "tensor(2.8512e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.8888e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.8670e-08, grad_fn=<SumBackward1>)\n",
      "tensor(6.5933e-08, grad_fn=<SumBackward1>)\n",
      "tensor(9.9857e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.5698e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.8924e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.5635e-08, grad_fn=<SumBackward1>)\n",
      "tensor(7.6313e-08, grad_fn=<SumBackward1>)\n",
      "tensor(6.0681e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.6864e-07, grad_fn=<SumBackward1>)\n",
      "tensor(8.6265e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.6332e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.5754e-08, grad_fn=<SumBackward1>)\n",
      "tensor(2.1758e-07, grad_fn=<SumBackward1>)\n",
      "tensor(5.7372e-07, grad_fn=<SumBackward1>)\n",
      "tensor(1.9135e-07, grad_fn=<SumBackward1>)\n",
      "tensor(2.7252e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.5990e-07, grad_fn=<SumBackward1>)\n",
      "tensor(4.2078e-07, grad_fn=<SumBackward1>)\n",
      "tensor(6.5571e-09, grad_fn=<SumBackward1>)\n",
      "tensor(2.6661e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.6758e-08, grad_fn=<SumBackward1>)\n",
      "tensor(1.7968e-07, grad_fn=<SumBackward1>)\n",
      "tensor(5.2915e-08, grad_fn=<SumBackward1>)\n",
      "tensor(5.2937e-09, grad_fn=<SumBackward1>)\n",
      "tensor(1.6476e-07, grad_fn=<SumBackward1>)\n",
      "tensor(7.6795e-09, grad_fn=<SumBackward1>)\n",
      "tensor(8.5304e-09, grad_fn=<SumBackward1>)\n",
      "tensor(1.9495e-08, grad_fn=<SumBackward1>)\n",
      "tensor(5.2523e-08, grad_fn=<SumBackward1>)\n",
      "tensor(6.5828e-08, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "VI = torch.inverse(cov(X)) #inverse of covariance matrix\n",
    "for _input in X:\n",
    "    print(_batch_mahalanobis(VI,_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
